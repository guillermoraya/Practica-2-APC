{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducció a la pràctica 2\n",
    "\n",
    "## Objectius\n",
    "\n",
    "Els objectius d'aquesta pràctica són:\n",
    "  \n",
    "* Aplicar models de classificació, ficant l'èmfasi en:\n",
    "    1. Aplicar diferents classificadors (regressor logístic i svm) i entendre les millores d'aplicar kernels.\n",
    "    2. Avaluar correctament l'error del model \n",
    "    3. Visualitzar les dades i el model resultant\n",
    "\n",
    "\n",
    "* Ésser capaç d'aplicar tècniques de classificació en casos reals\n",
    "\n",
    "* Validar els resultats en dades reals\n",
    "\n",
    "* Fomentar la capacitat per presentar resultats tècnics d'aprenentatge computacional de forma adequada davant altres persones\n",
    "\n",
    "\n",
    "## Bases de dades\n",
    "\n",
    "Cada grup utilitzarà les bases de dades que se li hagin assignat depenent del grup on s'ha apuntat al caronte. \n",
    "\n",
    "\n",
    "| # | GRUP | BASE DE DADES ASSIGNADA|\n",
    "|:-:|:-:|:--|\n",
    "|\t1\t|\tGA\\*01-0000\t| https://www.kaggle.com/rounakbanik/pokemon\t|\n",
    "|\t2\t|\tGA\\*02-0000\t| https://www.kaggle.com/c/titanic/data\t|\n",
    "|\t3\t|\tGA\\*03-0000\t| https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\t|\n",
    "|\t4\t|\tGA\\*04-0000\t|https://www.kaggle.com/iabhishekofficial/mobile-price-classification\t|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Avaluació i entregues de la pràctica 2\n",
    "\n",
    "En la pràctica 2, es presenten diversos problemes per comprendre els mètodes de classificació numèrica.\n",
    "\n",
    "Les entregues s'organitzen en dos nivells d'assoliment dels objectius, incrementals: apartat **B, (sobre 6 punts)**, assoliment baix; apartat **A, (sobre 4 punts)**, assoliment alt. La suma dels 2 apartats serà la nota final de la pràctica 2. Per a realitzar el apartat A, prèviament s'ha d'haver resolt l'apartat B.\n",
    "\n",
    "Per cada apartat s'utilitzarà una base de dades diferent. A l'apartat B, treballarem majoritariament amb dades numèriques i es farà servir per establir les bases i l'esquelet per l'apartat A, on hi trobarem unes dades molt més riques i complexes.\n",
    "\n",
    "Similarment a la sessió de de treball de la pràctica 1, a la sessió de treball es molt recomanat que pregunteu sobre les bases de dades assignades, els problemes que heu de resoldre, per si hi haguéssin errors amb les llibreries o les seves funcions, aclaracions sobre les preguntes a contestar i els metodes a aplicar...\n",
    "\n",
    "Així, aquesta sessió de treball està orientada a que, els alumnes que vingueu pugueu preguntar i resoldre dubtes sobre les bases de dades que us han estat assignades, preguntar sobre l'objectiu de cada apartat dels enunciats que no us hagi quedat clar, i preguntar sobre els resultats que esteu obtenint a l'hora d'analitzar les dades. A més, podreu veure com els vostres companys estan resolent altres bases de dades, per agafar idees i veure com altres problemes es poden solucionar amb els mètodes que heu vist a classe de teoria.\n",
    "\n",
    "I en la següent sessió del 25 de novembre s'evaluarà la **pràctica sencera amb els dos apartats**. Caldrà pujar al Caronte abans de les 00:59 del dimecres 24 de novembre un ZIP amb el codi, la documentació i el ppt (10 minuts).\n",
    "\n",
    "   * Entrega (Apartat B 6pts + Apartat A 4pts)\n",
    "     1. Memòria en format article explicant els resultats trobats sobre la bases de dades de l'apartat B i els experiments realitzats sobre la base de dades A (10-50 pàgs). (4pts + 2.5pts)\n",
    "     2. Codi python desenvolupat. (1.5pts + 1pts)\n",
    "     3. Presentació amb els resultats 4 min màxim. (0.5pts + 0.5pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartat (B): Comparativa de models (4pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Take the first two features. We could avoid this by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "n_classes = 3\n",
    "    \n",
    "fig, sub = plt.subplots(1, 2, figsize=(16,6))\n",
    "sub[0].scatter(X[:,0], y, c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "sub[1].scatter(X[:,1], y, c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "\n",
    "\n",
    "particions = [0.5, 0.7, 0.8]\n",
    "\n",
    "for part in particions:\n",
    "    x_t, x_v, y_t, y_v = train_test_split(X, y, train_size=part)\n",
    "    \n",
    "    #Creem el regresor logístic\n",
    "    logireg = LogisticRegression(C=2.0, fit_intercept=True, penalty='l2', tol=0.001)\n",
    "\n",
    "    # l'entrenem\n",
    "    logireg.fit(x_t, y_t)\n",
    "\n",
    "    print (\"Percentage of correct classification of  Logistic model,  training with \", part, \"% of the data: \", logireg.score(x_v, y_v),\"%\")\n",
    "    \n",
    "    #Creem el regresor logístic\n",
    "    svc = svm.SVC(C=10.0, kernel='rbf', gamma=0.9, probability=True)\n",
    "\n",
    "    # l'entrenem \n",
    "    svc.fit(x_t, y_t)\n",
    "    probs = svc.predict_proba(x_v)\n",
    "    print (\"Percentage of correct classification of  SVM model,       training with \", part, \"% of the data: \", svc.score(x_v, y_v),\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal com podeu llegir a [l'API de sklearn](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html), en comptes de fer una corba per cada classe, podem considerar totes les classes en conjunt en una sola corba (1 si hem predit la classe correcta, 0 si no). Això es coneix com a `micro-averaging`. \n",
    "\n",
    "Així, veureu que la funció `f1_score` utilitza el paràmetre `macro` per calcular la precision-recall-f1 per clase, i després fer la mitja pr a totes les classes; i `micro` per utilitzar totes les prediccions (i errors de FN, FP) per a calcular una única precision-recall-f1 per a totes les classes juntes.\n",
    "\n",
    "Si voleu calcular la corba Precision-Recall quan utilitzeu el K-fold, cal calcular les corbes per a cada fold i després [fer la mitja de tots els folds per obtenir la corba final](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py). En el cas del LOOCV no té sentit fer la mitja la corba PR perquè hauriem de fer servir totes les mostres com a $y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_curve, average_precision_score, roc_curve, auc\n",
    "\n",
    "# Compute Precision-Recall and plot curve\n",
    "precision = {}\n",
    "recall = {}\n",
    "average_precision = {}\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_v == i, probs[:, i])\n",
    "    average_precision[i] = average_precision_score(y_v == i, probs[:, i])\n",
    "\n",
    "    plt.plot(recall[i], precision[i],\n",
    "    label='Precision-recall curve of class {0} (area = {1:0.2f})'\n",
    "                           ''.format(i, average_precision[i]))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "\n",
    "    \n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_v == i, probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})' ''.format(i, roc_auc[i]))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "def show_C_effect(C=1.0, gamma=0.7, degree=3):\n",
    "\n",
    "    # import some data to play with\n",
    "    iris = datasets.load_iris()\n",
    "    # Take the first two features. We could avoid this by using a two-dim dataset\n",
    "    X = iris.data[:, :2]\n",
    "    y = iris.target\n",
    "\n",
    "    # we create an instance of SVM and fit out data. We do not scale our\n",
    "    # data since we want to plot the support vectors\n",
    "    # title for the plots\n",
    "    titles = ('SVC with linear kernel',\n",
    "              'LinearSVC (linear kernel)',\n",
    "              'SVC with RBF kernel',\n",
    "              'SVC with polynomial (degree 3) kernel')\n",
    "\n",
    "    #C = 1.0  # SVM regularization parameter\n",
    "    models = (svm.SVC(kernel='linear', C=C),\n",
    "              svm.LinearSVC(C=C, max_iter=1000000),\n",
    "              svm.SVC(kernel='rbf', gamma=gamma, C=C),\n",
    "              svm.SVC(kernel='poly', degree=degree, gamma='auto', C=C))\n",
    "    models = (clf.fit(X, y) for clf in models)\n",
    "\n",
    "    plt.close('all')\n",
    "    fig, sub = plt.subplots(2, 2, figsize=(14,9))\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "    for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "        plot_contours(ax, clf, xx, yy,\n",
    "                      cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xlabel('Sepal length')\n",
    "        ax.set_ylabel('Sepal width')\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    fig.suptitle('Plots for different models with C='+str(C))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podeu provar quin efecte té diferents valors de regularització per aquest petit exemple ( C=0.0001 to 1000..). També podeu veure com afecta els valors de degree i gamma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values=[0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "i=0\n",
    "show_C_effect(C=C_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_C_effect(C=C_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_C_effect(C=C_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_C_effect(C=C_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_C_effect(C=C_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_C_effect(C=C_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_C_effect(C=C_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_C_effect(C=C_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gamma_effect(C=1.0, gamma=0.7, degree=3):\n",
    "\n",
    "    # import some data to play with\n",
    "    iris = datasets.load_iris()\n",
    "    # Take the first two features. We could avoid this by using a two-dim dataset\n",
    "    X = iris.data[:, :2]\n",
    "    y = iris.target\n",
    "\n",
    "    # we create an instance of SVM and fit out data. We do not scale our\n",
    "    # data since we want to plot the support vectors\n",
    "    # title for the plots\n",
    "    titles = ('SVC with RBF kernel',\n",
    "              'SVC with polynomial (degree 3) kernel')\n",
    "\n",
    "    #C = 1.0  # SVM regularization parameter\n",
    "    models = (svm.SVC(kernel='rbf', gamma=gamma, C=C),\n",
    "              svm.SVC(kernel='poly', degree=degree, gamma=gamma, C=C))\n",
    "    models = (clf.fit(X, y) for clf in models)\n",
    "\n",
    "    plt.close('all')\n",
    "    fig, sub = plt.subplots(1, 2, figsize=(14,4.5))\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "    for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "        plot_contours(ax, clf, xx, yy,\n",
    "                      cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xlabel('Sepal length')\n",
    "        ax.set_ylabel('Sepal width')\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(title)\n",
    "\n",
    "        \n",
    "    fig.suptitle('Plots for different models with gamma='+str(gamma))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_values=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0]\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=gamma_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gamma_effect(gamma=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_degree_effect(C=1.0, gamma=0.7, degree=3):\n",
    "\n",
    "    # import some data to play with\n",
    "    iris = datasets.load_iris()\n",
    "    # Take the first two features. We could avoid this by using a two-dim dataset\n",
    "    X = iris.data[:, :2]\n",
    "    y = iris.target\n",
    "\n",
    "    # we create an instance of SVM and fit out data. We do not scale our\n",
    "    # data since we want to plot the support vectors\n",
    "    # title for the plots\n",
    "    title = 'SVC with polynomial (degree '+str(degree)+') kernel'\n",
    "\n",
    "    #C = 1.0  # SVM regularization parameter\n",
    "    model = svm.SVC(kernel='poly', degree=degree, gamma='auto', C=C)\n",
    "    model = model.fit(X, y)\n",
    "\n",
    "    plt.close('all')\n",
    "    fig = plt.figure(figsize=(14,9))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "    \n",
    "    plot_contours(ax, model, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    \n",
    "    plt.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('Sepal length')\n",
    "    ax.set_ylabel('Sepal width')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_values=[0,1,2,3,4,5,6,7,8,9,10]\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_degree_effect(degree=degree_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_degree_effect(degree=degree_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_degree_effect(degree=degree_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_degree_effect(degree=degree_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_degree_effect(degree=degree_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_degree_effect(degree=degree_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_degree_effect(degree=degree_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_degree_effect(degree=degree_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_degree_effect(degree=degree_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_degree_effect(degree=degree_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_degree_effect(degree=degree_values[i])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartat (A): Classificació Numèrica (6pts)\n",
    "\n",
    "Per a aquest primer apartat, s'analitzarà els tipus d'atributs que es tenen i, si no està estipulat, **caldrà fixar quin és l'atribut objectiu a classificar de tots els que hi ha a la base de dades**.\n",
    "Expliqueu a la memòria quin atribut heu fet servir, no hi ha una decisió única correcta, cal que doneu raons de per què heu triat l'atribut que hàgiu triat.\n",
    "\n",
    "Treballarem varis aspectes de la classificació:\n",
    "\n",
    "1. EDA (exploratory data analysis)\n",
    "2. Preprocessing (normalitzation, outlier removal, feature selection..)\n",
    "3. Model Selection\n",
    "4. Crossvalidation\n",
    "5. Metric Analysis\n",
    "6. Hyperparameter Search\n",
    "\n",
    "\n",
    "Durant els següents apartats, es recomana anar fent una taula amb el mètode, paràmetres i precisió obtinguda. D'aquesta manera serà més fàcil entendre i valorar què s'aconsegueix en cada metode. Exemple:\n",
    "\n",
    "<img src=\"images/table_1.png\" width=\"80%\">\n",
    "\n",
    "Les preguntes de cada apartat són orientatives. **NO** cal contestar-les totes, ni totes tindrán sentit per tots els datasets. Són una guia per a que reflexioneu i aprengueu detalls de cada apartat. Tot i no ser obligatories, si que són molt recomenades d'intentar respondre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcio per a llegir dades en format csv\n",
    "import pandas as pd\n",
    "def load_dataset(path):\n",
    "    dataset = pd.read_csv(path, header=0, delimiter=',')\n",
    "    return dataset\n",
    "\n",
    "# Carreguem dataset d'exemple\n",
    "df = load_dataset('pokemon.csv')\n",
    "\n",
    "# Fem una correció al dataset\n",
    "df.at[773, 'capture_rate'] = '30'\n",
    "df.at[773, 'attack'] = np.int64(60)\n",
    "df.at[773, 'defense'] = np.int64(100)\n",
    "df.at[773, 'sp_attack'] = np.int64(60)\n",
    "df.at[773, 'sp_defense'] = np.int64(100)\n",
    "df.at[773, 'speed'] = np.int64(60)\n",
    "df.at[773, 'base_total'] = np.int64(440)\n",
    "\n",
    "df.at[719, 'classfication'] = 'Mischief Pokémon'\n",
    "df.at[719, 'attack'] = np.int64(110)\n",
    "df.at[719, 'defense'] = np.int64(60)\n",
    "df.at[719, 'sp_attack'] = np.int64(150)\n",
    "df.at[719, 'sp_defense'] = np.int64(130)\n",
    "df.at[719, 'speed'] = np.int64(70)\n",
    "df.at[719, 'base_total'] = np.int64(600)\n",
    "df.at[719, 'height_m'] = np.int64(0.5)\n",
    "df.at[719, 'weight_kg'] = np.int64(9)\n",
    "\n",
    "\n",
    "df['capture_rate'] = df['capture_rate'].astype(np.int64)\n",
    "df['abilities'] = df['abilities'].apply(lambda x: eval(x))\n",
    "\n",
    "data = df.values\n",
    "labels = df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. EDA (exploratory data analysis)\n",
    "\n",
    "Igual com a la pràctica anterior, exploreu i visualitzeu com és la base de dades que teniu assignada.\n",
    "\n",
    "**Preguntes:**\n",
    "* Quants atributs té la vostra base de dades?\n",
    "    * _**Resposta:** La nostra base de dades té 41 atributs._\n",
    "* Quin tipus d'atributs tens? (Númerics, temporals, categorics, binaris...)\n",
    "    * _**Resposta:** Els tipus d'atributs que tenim són els següents:_\n",
    "    | Atribut | Tipus | Descripció |\n",
    "    | :-- | :-: | :-- |\n",
    "    |name| String |The English name of the Pokemon|\n",
    "    |japanese_name| String |The Original Japanese name of the Pokemon|\n",
    "    |pokedex_number| Numeric |The entry number of the Pokemon in the National Pokedex|\n",
    "    |percentage_male| Numeric. |The percentage of the species that are male. Blank if the Pokemon is genderless.|\n",
    "    |type1| String, categoric |The Primary Type of the Pokemon|\n",
    "    |type2| String, categoric |The Secondary Type of the Pokemon|\n",
    "    |classification| String, categoric |The Classification of the Pokemon as described by the Sun and Moon Pokedex|\n",
    "    |height_m| Numeric |Height of the Pokemon in metres|\n",
    "    |weight_kg| Numeric |The Weight of the Pokemon in kilograms|\n",
    "    |capture_rate| Numeric |Capture Rate of the Pokemon|\n",
    "    |baseeggsteps| Numeric |The number of steps required to hatch an egg of the Pokemon|\n",
    "    |abilities| String, categoric |A stringified list of abilities that the Pokemon is capable of having|\n",
    "    |experience_growth| Numeric |The Experience Growth of the Pokemon|\n",
    "    |base_happiness| Numeric |Base Happiness of the Pokemon|\n",
    "    |against_?| Numeric |Eighteen features that denote the amount of damage taken against an attack of a particular type|\n",
    "    |hp| Numeric |The Base HP of the Pokemon|\n",
    "    |attack| Numeric |The Base Attack of the Pokemon|\n",
    "    |defense| Numeric |The Base Defense of the Pokemon|\n",
    "    |sp_attack| Numeric |The Base Special Attack of the Pokemon|\n",
    "    |sp_defense| Numeric |The Base Special Defense of the Pokemon|\n",
    "    |speed| Numeric |The Base Speed of the Pokemon|\n",
    "    |generation| Numeric |The numbered generation which the Pokemon was first introduced|\n",
    "    |is_legendary| Binary |Denotes if the Pokemon is legendary|\n",
    "  \n",
    "* Com es el target, quantes categories diferents existeixen?\n",
    "    * _**Resposta:** El nostre target és 'is_legendary', i, com que és una dada binària, té dues categories (una per a Pokemon legendaris, l'altra per a no legendaris)._\n",
    "\n",
    "* Podeu veure alguna correlació entre X i y?\n",
    "    * _**Resposta:** Sí._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Treiem els atributs de against_? i els dels noms\n",
    "dropped = list(labels[1:19]) + list(labels[29:31])\n",
    "df_mod = df.drop(columns=[l for l in dropped])\n",
    "correlacio = df_mod.corr()\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Mirem la correlació entre els atributs per entendre millor les dades\n",
    "ax = sns.heatmap(correlacio, annot=True, linewidths=.5, center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = df_mod.columns.values\n",
    "\n",
    "for i in [5, 4, 10, 1, 7, 13, 14, 15, 9, 18]:\n",
    "    plt.subplots(figsize=(10, 10))\n",
    "    sns.histplot(df_mod, x=labels[i], hue=labels[20], multiple=\"layer\", element=\"poly\")\n",
    "    plt.show()\n",
    "\n",
    "for i in [2, 3, 8, 12, 19]:\n",
    "    plt.subplots(figsize=(10, 10))\n",
    "    sns.histplot(df_mod, x=labels[i], hue=labels[20], multiple=\"stack\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Estan balancejades les etiquetes (distribució similar entre categories)? Creus que pot afectar a la classificació la seva distribució?\n",
    "    * _**Resposta:** No, no estan gens balancejades. Hi ha molt pocs legendaris, i això pot esbiaixar la classificació (un classificador que digui \"no legendari\" per a qualsevol pokemon tindrà una bona accuracy, i això és inadmissible)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing (normalitzation, outlier removal, feature selection..)\n",
    "Un cop vistes les dades de les que es disposa, per tal de tenir un aprenentatge més eficient, es recomana normalitzar les dades i treure outliers. Segons la tipologia de dades, es poden filtrar atributs, aplicar-hi reductors de dimensionalitat, codificar categories textuals en valors numèrics..\n",
    "\n",
    "Navegueu per la [documentació de sklearn sobre preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) per tal de trobar les diferents opcions que proporciona sklearn.\n",
    "\n",
    "**Preguntes:**\n",
    "* Estàn les dades normalitzades? Caldria fer-ho?\n",
    "    * _**Resposta:** No, les dades no estan normalitzades: l'escala amb que es mesuren els atributs no és la mateixa per a tots. Pensem que serà convenient normalitzar les dades._\n",
    "* En cas que les normalitzeu, quin tipus de normalització será més adient per les vostres dades?\n",
    "    * _**Resposta:** Per assegurar-nos de que totes les dades estan normalitzades i no complicar-nos massa aplicarem a tots la mateixa normalitzaci ́o, el StandardScaler que transforma les dades perquè la mitja sigui 0 i la variància 1._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Teniu gaires dades sense informació? Els NaNs a pandas? Tingueu en compte que hi ha metodes que no els toleren durant el aprenentatge. Com afecta a la classificació si les filtrem? I si les reompliu? Com ho farieu? [Pista](https://scikit-learn.org/stable/modules/impute.html)\n",
    "    * _**Resposta:** Sí, tenim un atribut on és molt freqüent trobar NaNs: \"percentage_male\". Aquests NaN tenen una explicació lògica: hi ha tot un conjunt de Pokémon que no tenen gènere (anomenats \"genderless\"). Hem aprofitat la presència d'aquests NaN per a crear un atribut \"genderless\", que val 1 per als casos on \"percentage_male\" és \"NaN\", i zero altrament. Aquesta variable sembla tenir força correlació amb el comportament de la variable objectiu \"is_legendary\". Un altre atribut que té \"NaN\" és `type_2`, ja que hi ha força pokemon que són d'un sol tipus. Hem provat a crear un atribut per a els Pokemon d'un sol tipus, però no té gaire correlació amb la variable objectiu. Els últims atributs amb \"Nan\" són `height_m` i `weight_kg` on apareixen els pokemons que tenen una forma alola alternativa. Aquest atribut té el valor \"Nan\" en aquests casos inclus quan les dades de les formes coincideixen._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra = pd.DataFrame()\n",
    "df_extra['genderless'] = df['percentage_male'].isnull() + 0\n",
    "df_extra['percentage_male'] = df['percentage_male']\n",
    "df_extra.at[df_extra['percentage_male'].isnull(), 'percentage_male'] = 0\n",
    "df_extra['percentage_female'] = 1 - df['percentage_male']\n",
    "df_extra.at[df_extra['percentage_female'].isnull(), 'percentage_female'] = 0\n",
    "df_extra['monotype'] = df['type2'].isnull() + 0\n",
    "df_extra['is_legendary'] = df['is_legendary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlacio = df_extra.corr()\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Mirem la correlació entre els atributs per entendre millor les dades\n",
    "ax = sns.heatmap(correlacio, annot=True, linewidths=.5, center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Teniu dades categoriques? Quina seria la codificació amb més sentit? (`OrdinalEncoder`, `OneHotEncoder`, d'altres?)\n",
    "    * _**Resposta:** Sí, tenim dades categòriques als atributs `type1`, `type2`, `classification` i `abilities`. Les variable `classification` i `abilities` no les farem servir ja que hi ha una quantita excesiva (per diverses mostres la seva categoria és única, per tant, el regressor ignoraria aquestes variables al training igualment) i no sembla aportar-nos gaire informació, i com que `type2` té força nulls (que indican que no té un segon tipus) i representa el mateix que `type1` l'ajuntarem amb aquesta per fer la codificació. Per tant, per a aquesta combinació, provarem a fer ús d'una codificació mitjant `OneHotEncoder`, ja que `OrdinalEncoder` pot donar problemes (trobar relacions espúries entre l'ordinal que representa cada categoria amb el comportament de la variable objectiu)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = np.unique(df['type1'])\n",
    "dt = pd.DataFrame(list(df.apply(lambda x: [(x.at['type1'] == t) * (x.at['type1'] == t)for t in types], axis=1)), columns=types)\n",
    "dt['is_legendary'] = df['is_legendary']\n",
    "    \n",
    "abilities = []\n",
    "for ability in df['abilities']:\n",
    "    abilities += ability\n",
    "abilities = np.unique(abilities)\n",
    "da = pd.DataFrame(list(df['abilities'].apply(lambda x: [(ability in x) + 0 for ability in abilities])), columns=abilities)\n",
    "da['is_legendary'] = df['is_legendary']\n",
    "\n",
    "classfication = np.unique(df['classfication'])\n",
    "dc = pd.DataFrame(list(df['classfication'].apply(lambda x: [(x == c) + 0 for c in classfication])), columns=classfication)\n",
    "dc['is_legendary'] = df['is_legendary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "correlacio = dt.corr()\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Mirem la correlació entre els atributs per entendre millor les dades\n",
    "ax = sns.heatmap(correlacio, annot=True, linewidth=.5, center=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(60, 18))\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ab = []\n",
    "il = []\n",
    "for ability, legendary in zip(df['abilities'], df['is_legendary']):\n",
    "    ab += ability\n",
    "    il += [legendary for _ in range(len(ability))]\n",
    "    \n",
    "sns.histplot(x=ab, hue=il, multiple=\"stack\")\n",
    "plt.show()\n",
    "print(f\"Numero d'abilitats diferents: {len(abilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(120, 18))\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "sns.histplot(x=df['classfication'], hue=df['is_legendary'], multiple=\"stack\")\n",
    "plt.show()\n",
    "print(f\"Numero de classificacions diferents: {len(classfication)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Caldria aplicar `sklearn.decomposition.PCA`? Quins beneficis o inconvenients trobarieu?\n",
    "    * _**Resposta:** No,  nom ́es usant 3 bons atributs ja podem fer classificacions quasi perfectes (com es veu m ́esendevant) aixi que no val la pena fer-ho.._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Selection\n",
    "La tasca d'aquesta pràctica s'enmarca dins l'aprenentatge computacional **supervisat**. A sklearn, disposem de varies tècniques [(veure documentació)](https://scikit-learn.org/stable/supervised_learning.html). A les classes de teoria, hem vist varies tècniques, com ara logistic regression, SVM amb diferents kernels, Nearest Neighbour, i el perceptró...\n",
    "En aquesta secció heu de valorar quina o quines tècniques voleu fer servir, aixi com també explicar el per què les heu seleccionat. Recomanem, que per entendre millor la teoria, s'ha de provar com a mínim un model de SVM.\n",
    "\n",
    "**Preguntes:**\n",
    "* Quins models heu considerat?\n",
    "* Considereu les SVM amb els diferents kernels implementats.\n",
    "* Quin creieu que serà el més precís?\n",
    "* Quin serà el més ràpid?\n",
    "* Seria una bona idea fer un `ensemble`? Quins inconvenients creieu que pot haver-hi? [Documentació](https://scikit-learn.org/stable/modules/ensemble.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.DataFrame()\n",
    "attrib_mod = ['base_egg_steps', 'base_happiness', 'base_total', 'capture_rate', \\\n",
    "             'experience_growth']\n",
    "df_full[attrib_mod] = df_mod[attrib_mod]\n",
    "attrib_extra = ['genderless']\n",
    "df_full[attrib_extra] = df_extra[attrib_extra]\n",
    "attrib_legendary = 'is_legendary'\n",
    "df_full[attrib_legendary] = df[attrib_legendary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probem amb els atributs:\n",
    "base_egg_steps, base_total,genderless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_full.values[:, [0, 2, 5]]\n",
    "y = df_full.values[:, 6]\n",
    "\n",
    "particions = [0.5, 0.7, 0.8]\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "C_Log_Reg = 2.\n",
    "C_SVM_rbf = 1. \n",
    "C_SVM_Lin = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in particions:\n",
    "    x_t, x_v, y_t, y_v = train_test_split(X_scaled, y, train_size=part, stratify=y)\n",
    "    \n",
    "    # Creem el regresor logístic\n",
    "    logireg = LogisticRegression(C=C_Log_Reg, fit_intercept=True, penalty='l2', tol=0.001)\n",
    "\n",
    "    # L'entrenem\n",
    "    logireg.fit(x_t, y_t)\n",
    "\n",
    "    print (\"Correct classification Logistic \", part*100, \"% of the data: \", logireg.score(x_v, y_v))\n",
    "    print(metrics.classification_report(y_v, logireg.predict(x_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in particions:\n",
    "    x_t, x_v, y_t, y_v = train_test_split(X_scaled, y, train_size=part, stratify=y)\n",
    "    \n",
    "    # Creem el SVC Lineal\n",
    "    svcLin = svm.LinearSVC(C=C_SVM_Lin, max_iter=1000000)\n",
    "    \n",
    "    # L'entrenem\n",
    "    svcLin.fit(x_t,y_t)\n",
    "    \n",
    "    print (\"\\n\\nCorrect classification SVM Linear \", part*100, \"% of the data: \", svcLin.score(x_v, y_v))\n",
    "    print(metrics.classification_report(y_v, svcLin.predict(x_v)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in particions:\n",
    "    x_t, x_v, y_t, y_v = train_test_split(X_scaled, y, train_size=part, stratify=y)\n",
    "    \n",
    "    # Creem el SVC rbf\n",
    "    svc = svm.SVC(C=C_SVM_rbf, kernel='rbf', gamma=0.9, probability=True)\n",
    "    \n",
    "    # L'entrenem \n",
    "    svc.fit(x_t, y_t)\n",
    "    \n",
    "    print (\"\\n\\nCorrect classification SVM rbf     \", part*100, \"% of the data: \", svc.score(x_v, y_v))\n",
    "    print(metrics.classification_report(y_v, svc.predict(x_v)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probem amb altres atributs:\n",
    "base_happines, experience_growth, capture_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_full.values[:, [1,3,4]]\n",
    "y = df_full.values[:, 6]\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in particions:\n",
    "    x_t, x_v, y_t, y_v = train_test_split(X_scaled, y, train_size=part, stratify=y)\n",
    "    \n",
    "    # Creem el regresor logístic\n",
    "    logireg = LogisticRegression(C=C_Log_Reg, fit_intercept=True, penalty='l2', tol=0.001)\n",
    "\n",
    "    # L'entrenem\n",
    "    logireg.fit(x_t, y_t)\n",
    "\n",
    "    print (\"Correct classification Logistic \", part*100, \"% of the data: \", logireg.score(x_v, y_v))\n",
    "    print(metrics.classification_report(y_v, logireg.predict(x_v)))\n",
    "    print(np.unique(logireg.predict(x_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in particions:\n",
    "    x_t, x_v, y_t, y_v = train_test_split(X_scaled, y, train_size=part, stratify=y)\n",
    "    \n",
    "    # Creem el SVC Lineal\n",
    "    svcLin = svm.LinearSVC(C=C_SVM_Lin, max_iter=1000000)\n",
    "    \n",
    "    # L'entrenem\n",
    "    svcLin.fit(x_t,y_t)\n",
    "    \n",
    "    print (\"\\n\\nCorrect classification SVM Linear \", part*100, \"% of the data: \", svcLin.score(x_v, y_v))\n",
    "    print(metrics.classification_report(y_v, svcLin.predict(x_v)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in particions:\n",
    "    x_t, x_v, y_t, y_v = train_test_split(X_scaled, y, train_size=part, stratify=y)\n",
    "    \n",
    "    # Creem el SVC rbf\n",
    "    svc = svm.SVC(C=C_SVM_rbf, kernel='rbf', gamma=0.9, probability=True)\n",
    "    \n",
    "    # L'entrenem \n",
    "    svc.fit(x_t, y_t)\n",
    "    \n",
    "    print (\"\\n\\nCorrect classification SVM rbf     \", part*100, \"% of the data: \", svc.score(x_v, y_v))\n",
    "    print(metrics.classification_report(y_v, svc.predict(x_v)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Crossvalidation\n",
    "Un cop seleccionats quins models es volen testejar sobre les dades, s'han de poder evaluar correctament. Per aquests motius, haurem d'aprendre a cros-validar els resultats.\n",
    "Reviseu la [documentació](https://scikit-learn.org/stable/modules/cross_validation.html) i escolliu quin tipus de crossvalidació pot ser l'adecuada pel vostre problema.\n",
    "\n",
    "**Preguntes:**\n",
    "* Per què és important cross-validar els resultats?\n",
    "* Separa la base de dades en el conjunt de train-test. Com de fiables serán els resultats obtinguts? En quins casos serà més fiable, si tenim moltes dades d'entrenament o poques?\n",
    "* Quin tipus de K-fold heu escollit? Quants conjunts heu seleccionat (quina k)? Com afecta els diferents valors de k?\n",
    "* Es viable o convenient aplicar `LeaveOneOut`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_full.values[:, [0, 2, 5]]\n",
    "y = df_full.values[:, 6]\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in particions:\n",
    "    x_t, x_v, y_t, y_v = train_test_split(X_scaled, y, train_size=part, stratify=y)\n",
    "    \n",
    "    # Creem el regresor logístic\n",
    "    logireg = LogisticRegression(C=C_Log_Reg, fit_intercept=True, penalty='l2', tol=0.001)\n",
    "\n",
    "    # L'entrenem\n",
    "    logireg.fit(x_t, y_t)\n",
    "    \n",
    "    cvs = cross_val_score(logireg, x_v, y_v)\n",
    "    print(\"Cross validation de partició \" + str(part*100) + \"%:\")\n",
    "    print(cvs)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in particions:\n",
    "    x_t, x_v, y_t, y_v = train_test_split(X_scaled, y, train_size=part, stratify=y)\n",
    "    \n",
    "    # Creem el SVC Lineal\n",
    "    svcLin = svm.LinearSVC(C=C_SVM_Lin, max_iter=1000000)\n",
    "    \n",
    "    # L'entrenem\n",
    "    svcLin.fit(x_t,y_t)\n",
    "    \n",
    "    cvs = cross_val_score(svc, x_v, y_v)\n",
    "    print(\"Cross validation de partició \" + str(part*100) + \"%:\")\n",
    "    print(cvs)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in particions:\n",
    "    x_t, x_v, y_t, y_v = train_test_split(X_scaled, y, train_size=part, stratify=y)\n",
    "    \n",
    "    # Creem el SVC rbf\n",
    "    svc = svm.SVC(C=C_SVM_rbf, kernel='rbf', gamma=0.9, probability=True)\n",
    "    \n",
    "    # L'entrenem \n",
    "    svc.fit(x_t, y_t)\n",
    "    \n",
    "    cvs = cross_val_score(svc, x_v, y_v)\n",
    "    print(\"Cross validation de partició \" + str(part*100) + \"%:\")\n",
    "    print(cvs)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probes amb K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferent Kfolds per la regressió logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "count=0\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    x_t, x_v = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_t, y_v = y[train_index], y[test_index]\n",
    "    logireg = LogisticRegression(C=C_Log_Reg, fit_intercept=True, penalty='l2', tol=0.001, max_iter=1000)\n",
    "\n",
    "    logireg.fit(x_t, y_t)\n",
    "\n",
    "    cvs = cross_val_score(logireg, x_v, y_v)\n",
    "    count += 1\n",
    "    print(\"Cross Validation del KFold: \" + str(count))\n",
    "    print(cvs)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4)\n",
    "count=0\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    x_t, x_v = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_t, y_v = y[train_index], y[test_index]\n",
    "    logireg = LogisticRegression(C=C_Log_Reg, fit_intercept=True, penalty='l2', tol=0.001, max_iter=1000)\n",
    "\n",
    "    logireg.fit(x_t, y_t)\n",
    "\n",
    "    cvs = cross_val_score(logireg, x_v, y_v)\n",
    "    count += 1\n",
    "    print(\"Cross Validation del KFold: \" + str(count))\n",
    "    print(cvs)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3)\n",
    "count = 0\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    x_t, x_v = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_t, y_v = y[train_index], y[test_index]\n",
    "    logireg = LogisticRegression(C=C_Log_Reg, fit_intercept=True, penalty='l2', tol=0.001, max_iter=1000)\n",
    "\n",
    "    logireg.fit(x_t, y_t)\n",
    "\n",
    "    cvs = cross_val_score(logireg, x_v, y_v)\n",
    "    count += 1\n",
    "    print(\"Cross Validation del KFold: \" + str(count))\n",
    "    print(cvs)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferent Kfolds per al SVM Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "count =0\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    x_t, x_v = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_t, y_v = y[train_index], y[test_index]\n",
    "    svcLin = svm.LinearSVC(C=C_SVM_Lin, max_iter=1000000)\n",
    "    svcLin.fit(x_t,y_t)\n",
    "    cvs = cross_val_score(svc, x_v, y_v)\n",
    "    count += 1\n",
    "    print(\"Cross Validation del KFold: \" + str(count))\n",
    "    print(cvs)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4)\n",
    "count =0\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    x_t, x_v = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_t, y_v = y[train_index], y[test_index]\n",
    "    svcLin = svm.LinearSVC(C=C_SVM_Lin, max_iter=1000000)\n",
    "    svcLin.fit(x_t,y_t)\n",
    "    cvs = cross_val_score(svc, x_v, y_v)\n",
    "    count += 1\n",
    "    print(\"Cross Validation del KFold: \" + str(count))\n",
    "    print(cvs)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3)\n",
    "count =0\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    x_t, x_v = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_t, y_v = y[train_index], y[test_index]\n",
    "    svcLin = svm.LinearSVC(C=C_SVM_Lin, max_iter=1000000)\n",
    "    svcLin.fit(x_t,y_t)\n",
    "    cvs = cross_val_score(svc, x_v, y_v)\n",
    "    count += 1\n",
    "    print(\"Cross Validation del KFold: \" + str(count))\n",
    "    print(cvs)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferent Kfolds per al SVM RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "count=0\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    x_t, x_v = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_t, y_v = y[train_index], y[test_index]\n",
    "    svc = svm.SVC(C=C_SVM_rbf, kernel='rbf', gamma=0.9, probability=True)\n",
    "    \n",
    "    svc.fit(x_t, y_t)\n",
    "    cvs = cross_val_score(svc, x_v, y_v)\n",
    "    count += 1\n",
    "    print(\"Cross Validation del KFold: \" + str(count))\n",
    "    print(cvs)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4)\n",
    "count=0\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    x_t, x_v = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_t, y_v = y[train_index], y[test_index]\n",
    "    svc = svm.SVC(C=C_SVM_rbf, kernel='rbf', gamma=0.9, probability=True)\n",
    "    \n",
    "    svc.fit(x_t, y_t)\n",
    "    cvs = cross_val_score(svc, x_v, y_v)\n",
    "    count += 1\n",
    "    print(\"Cross Validation del KFold: \" + str(count))\n",
    "    print(cvs)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3)\n",
    "count =0\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    x_t, x_v = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_t, y_v = y[train_index], y[test_index]\n",
    "    svc = svm.SVC(C=C_SVM_rbf, kernel='rbf', gamma=0.9, probability=True)\n",
    "    \n",
    "    svc.fit(x_t, y_t)\n",
    "    cvs = cross_val_score(svc, x_v, y_v)\n",
    "    count += 1\n",
    "    print(\"Cross Validation del KFold: \" + str(count))\n",
    "    print(cvs)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Metric Analysis\n",
    "En aquest apartat ens centrarem en les mètriques de classificació ([documentació](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)).\n",
    "\n",
    "**Preguntes:**\n",
    "* A teoria, hem vist el resultat d'aplicar el `accuracy_score` sobre dades no balancejades. Podrieu explicar i justificar quina de les següents mètriques será la més adient pel vostre problema? `accuracy_score`, `f1_score` o `average_precision_score`.\n",
    "* Mostreu la Precisió-Recall Curve i la ROC Curve. Quina és més rellevant pel vostre dataset? Expliqueu amb les vostres paraules, la diferencia entre una i altre [Pista](https://stats.stackexchange.com/questions/338826/auprc-vs-auc-roc)\n",
    "* Què mostra [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)? Quina métrica us fixareu per tal de optimitzar-ne la classificació pel vostre cas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = .7\n",
    "x_t, x_v, y_t, y_v = train_test_split(X_scaled, y, train_size=part, stratify=y)\n",
    "\n",
    "# Creem el regresor logístic\n",
    "logireg = LogisticRegression(C=C_Log_Reg, fit_intercept=True, penalty='l2', tol=0.001)\n",
    "\n",
    "# L'entrenem\n",
    "logireg.fit(x_t, y_t)\n",
    "\n",
    "print(metrics.classification_report(y_v, logireg.predict(x_v)))\n",
    "plt.figure()\n",
    "metrics.plot_precision_recall_curve(logireg,x_v,y_v, color = 'green')\n",
    "metrics.plot_roc_curve(logireg,x_v,y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t, x_v, y_t, y_v = train_test_split(X_scaled, y, train_size=part, stratify=y)\n",
    "\n",
    "# Creem el SVC Lineal\n",
    "svcLin = svm.LinearSVC(C=C_SVM_Lin, max_iter=1000000)\n",
    "\n",
    "# L'entrenem\n",
    "svcLin.fit(x_t,y_t)\n",
    "\n",
    "print(metrics.classification_report(y_v, svcLin.predict(x_v)))\n",
    "plt.figure()\n",
    "metrics.plot_precision_recall_curve(svcLin,x_v,y_v, color='green')\n",
    "metrics.plot_roc_curve(svcLin,x_v,y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t, x_v, y_t, y_v = train_test_split(X_scaled, y, train_size=part, stratify=y)\n",
    "\n",
    "# Creem el SVC rbf\n",
    "svc = svm.SVC(C=C_SVM_rbf, kernel='rbf', gamma=0.9, probability=True)\n",
    "\n",
    "# L'entrenem \n",
    "svc.fit(x_t, y_t)\n",
    "\n",
    "print(metrics.classification_report(y_v, svc.predict(x_v)))\n",
    "plt.figure()\n",
    "metrics.plot_precision_recall_curve(svc,x_v,y_v, color='green')\n",
    "metrics.plot_roc_curve(svc,x_v,y_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Hyperparameter Search\n",
    "El motiu d'aplicar crossvalidació durant l'entrenament és que ens permet conèixer quin serà el resultat esperat del nostre model un cop en producció, és a dir, com es comportarà sobre dades mai vistes abans.\n",
    "A més, també ens permet optimitzar quins són els hiperparametres dels models que millor funcionaran en el futur test.\n",
    "\n",
    "**Preguntes:**\n",
    "* Quines formes de buscar el millor parametre heu trobat? Són costoses computacionalment parlant? [documentació](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "* Si disposem de recursos limitats (per exemple, un PC durant 1 hora) quin dels dos métodes creieu que obtindrà millor resultat final?\n",
    "* Existeixen altres mètodes de búsqueda més eficients ([scikit-optimize](https://scikit-optimize.github.io/stable/))?\n",
    "* Feu la prova, i amb el model i el metode de crossvalidació escollit, configureu els diferents metodes de búsqueda per a que s'executin durant el mateix temps (i.e. depenent del problema, 0,5h-1 hora). Analitzeu quin ha arribat a una millor solució. (estimeu el temps que trigarà a fer 1 training, i aixi trobeu el número de intents que podeu fer en cada cas.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def qualitat(C, tol):\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "    f1 = 0\n",
    "    for train_index, test_index in kf.split(X_scaled, y):\n",
    "        x_t, x_v = X_scaled[train_index], X_scaled[test_index]\n",
    "        y_t, y_v = y[train_index], y[test_index]\n",
    "        logireg = LogisticRegression(C=C, fit_intercept=True, penalty='l2', tol=tol, max_iter=1000)\n",
    "\n",
    "        logireg.fit(x_t, y_t)\n",
    "\n",
    "        f1 += metrics.classification_report(y_v, logireg.predict(x_v), output_dict=True)['macro avg']['f1-score']\n",
    "    return f1 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "npts = 200\n",
    "Cs = np.random.uniform(-5, 3, npts)\n",
    "tols = np.random.uniform(-8, -3, npts)\n",
    "\n",
    "z = np.array([qualitat(10 ** C, 10 ** tol) for C, tol in zip(Cs, tols)])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1)\n",
    "ax.tricontour(Cs, tols, z, levels=14, linewidths=0.5, colors='k')\n",
    "cntr2 = ax.tricontourf(Cs, tols, z, levels=14, cmap=\"RdBu_r\")\n",
    "\n",
    "fig.colorbar(cntr2, ax=ax)\n",
    "ax.plot(Cs, tols, 'ko', ms=3)\n",
    "ax.set(xlim=(-4.5, 2.5), ylim=(-7.5, -3.5))\n",
    "ax.set_title('tricontour (%d points)' % npts)\n",
    "ax.set_ylabel('log(tolerance)')\n",
    "ax.set_xlabel('log(C)')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "npts = 10\n",
    "Cs, tols = np.meshgrid(np.linspace(-5, 3, npts),\n",
    "                       np.linspace(-8, -3, npts))\n",
    "\n",
    "z = np.array([qualitat(10 ** C, 10 ** tol) for C, tol in zip(Cs.ravel(), tols.ravel())])\n",
    "z = z.reshape(Cs.shape)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1)\n",
    "ax.contour(Cs, tols, z, linewidths=0.5, colors='k')\n",
    "cntr2 = ax.contourf(Cs, tols, z, cmap=\"RdBu_r\")\n",
    "\n",
    "fig.colorbar(cntr2, ax=ax)\n",
    "ax.set(xlim=(-5, 3), ylim=(-8, -3))\n",
    "ax.set_title('contour (%d points)' % npts)\n",
    "ax.set_ylabel('log(tolerance)')\n",
    "ax.set_xlabel('log(C)')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@article{scikit-learn,\n",
    " title={Scikit-learn: Machine Learning in {P}ython},\n",
    " author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n",
    "         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n",
    "         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n",
    "         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n",
    " journal={Journal of Machine Learning Research},\n",
    " volume={12},\n",
    " pages={2825--2830},\n",
    " year={2011}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
